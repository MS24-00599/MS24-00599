{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23baf64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import package\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from transformers import BertModel, BertPreTrainedModel, AdamW, BertConfig, LongformerForMaskedLM, LongformerModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertModel, DistilBertPreTrainedModel\n",
    "\n",
    "from torch.nn import CrossEntropyLoss,MSELoss,BCELoss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from scipy.special import logit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9538a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 123\n"
     ]
    }
   ],
   "source": [
    "#set random seed globally\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    #random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf36b920-ac03-423f-ace5-9a083cdbf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalBert v2: Enhanced BERT-based model for causal inference with confounder integration\n",
    "# This version directly incorporates confounder variables into the neural network architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss, BCELoss, MSELoss\n",
    "from transformers import LongformerModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import logit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global configuration\n",
    "CUDA = (torch.cuda.device_count() > 0)  # Check if CUDA GPU is available\n",
    "MASK_IDX = 103  # Token ID for [MASK] token in Longformer tokenizer\n",
    "\n",
    "\n",
    "def platt_scale(outcome, probs):\n",
    "    \"\"\"\n",
    "    Apply Platt scaling for probability calibration.\n",
    "    Transforms raw model outputs to well-calibrated probabilities using logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        outcome: Ground truth binary outcomes [n_samples]\n",
    "        probs: Predicted probabilities to be calibrated [n_samples]\n",
    "    \n",
    "    Returns:\n",
    "        Calibrated probabilities using fitted logistic regression\n",
    "    \"\"\"\n",
    "    logits = logit(probs)\n",
    "    logits = logits.reshape(-1, 1)\n",
    "    log_reg = LogisticRegression(penalty='none', warm_start=True, solver='lbfgs')\n",
    "    log_reg.fit(logits, outcome)\n",
    "    return log_reg.predict_proba(logits)\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Unit (GELU) activation function.\n",
    "    Smoother alternative to ReLU that provides better gradient flow.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "    \n",
    "    Returns:\n",
    "        GELU-activated tensor: 0.5 * x * (1 + erf(x/√2))\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def make_bow_vector(ids, vocab_size, use_counts=False):\n",
    "    \"\"\"\n",
    "    Convert dense token IDs to sparse bag-of-words representation.\n",
    "    Useful for creating categorical feature representations from token sequences.\n",
    "    \n",
    "    Args:\n",
    "        ids: torch.LongTensor [batch, features] - Dense tensor of token IDs\n",
    "        vocab_size: Size of vocabulary for creating BOW vector\n",
    "        use_counts: If True, use token frequency counts; if False, use binary indicators\n",
    "    \n",
    "    Returns:\n",
    "        Sparse bag-of-words representation [batch, vocab_size]\n",
    "    \"\"\"\n",
    "    vec = torch.zeros(ids.shape[0], vocab_size)\n",
    "    ones = torch.ones_like(ids, dtype=torch.float)\n",
    "    \n",
    "    # Move tensors to GPU if available\n",
    "    if CUDA:\n",
    "        vec = vec.cuda()\n",
    "        ones = ones.cuda()\n",
    "        ids = ids.cuda()\n",
    "\n",
    "    # Create BOW vector using scatter_add for efficient sparse operations\n",
    "    vec.scatter_add_(1, ids, ones)\n",
    "    vec[:, 1] = 0.0  # Zero out padding tokens (assuming pad token ID is 1)\n",
    "    \n",
    "    # Convert to binary indicators if not using counts\n",
    "    if not use_counts:\n",
    "        vec = (vec != 0).float()\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "class CausalBert(LongformerModel):\n",
    "    \"\"\"\n",
    "    CausalBert v2: Enhanced Longformer-based model for causal inference with confounder integration.\n",
    "    \n",
    "    Key improvements over v1:\n",
    "    1. Direct integration of confounder variables into neural network layers\n",
    "    2. Enhanced architecture that combines text representations with structured confounders\n",
    "    3. Improved handling of selection bias through explicit confounder modeling\n",
    "    \n",
    "    The model learns:\n",
    "    1. Rich text representations using pretrained Longformer\n",
    "    2. Propensity scores considering both text and confounders\n",
    "    3. Potential outcomes under different treatments (Q-learning) with confounder adjustment\n",
    "    4. Masked language modeling as auxiliary task for representation learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # load longformer from input configs\n",
    "        self.encoder = LongformerModel(config)\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        self.vocab_size = config.vocab_size\n",
    "        \n",
    "        # Masked Language Modeling components\n",
    "        # These layers project hidden states back to vocabulary space for MLM loss\n",
    "        self.vocab_transform = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.vocab_projector = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "        # Q-learning heads: Predict potential outcomes for each treatment\n",
    "        # KEY CHANGE: Input size is now hidden_size + 2 to accommodate confounders\n",
    "        # The +2 suggests confounders are 2-dimensional (e.g., [age, income] or [feature1, feature2])\n",
    "        self.Q_cls = nn.ModuleDict()\n",
    "        for T in range(2):  # Binary treatment (T=0, T=1)\n",
    "            self.Q_cls['%d' % T] = nn.Sequential(\n",
    "                nn.Linear(config.hidden_size + 2, 200),  # Text features + 2 confounder dimensions (uses can change the number of additional confounders)\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, 1)  # Single output for outcome prediction\n",
    "            )\n",
    "\n",
    "        # Propensity score network: Predict P(T=1|text,confounders)\n",
    "        # Also takes concatenated input of text representations and confounders\n",
    "        self.g_cls = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size + 2, 1),  # Text + confounder features\n",
    "            nn.Sigmoid()  # Output probability between 0 and 1\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, W_ids, W_len, W_mask, C, T, Y=None, use_mlm=True):\n",
    "        \"\"\"\n",
    "        Forward pass of CausalBert with confounder integration.\n",
    "        \n",
    "        Args:\n",
    "            W_ids: Token IDs for input text [batch_size, seq_len]\n",
    "            W_len: Actual lengths of sequences [batch_size]\n",
    "            W_mask: Attention mask [batch_size, seq_len]\n",
    "            C: Confounder variables [batch_size, 2] - structured features like age, income, etc.\n",
    "            T: Treatment indicators [batch_size] - binary treatment assignment\n",
    "            Y: Outcomes [batch_size] (optional, for training)\n",
    "            use_mlm: Whether to use masked language modeling auxiliary task\n",
    "        \n",
    "        Returns:\n",
    "            g: Propensity scores [batch_size, 1]\n",
    "            Q0: Predicted outcomes under treatment 0 [batch_size, 1]\n",
    "            Q1: Predicted outcomes under treatment 1 [batch_size, 1]\n",
    "            g_loss: Propensity score prediction loss\n",
    "            Q_loss: Q-learning outcome prediction loss\n",
    "            mlm_loss: Masked language modeling loss\n",
    "        \"\"\"\n",
    "        \n",
    "        # Masked Language Modeling setup for auxiliary learning\n",
    "        if use_mlm:\n",
    "            W_len = W_len.unsqueeze(1) - 2  # Subtract 2 for [CLS] and [SEP] special tokens\n",
    "            mask_class = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "            \n",
    "            # Randomly select positions to mask (avoiding [CLS] token at position 0)\n",
    "            mask = (mask_class(W_len.shape).uniform_() * W_len.float()).long() + 1\n",
    "            target_words = torch.gather(W_ids, 1, mask)\n",
    "            \n",
    "            # Create MLM labels: -100 for positions we don't want to predict\n",
    "            mlm_labels = torch.ones(W_ids.shape).long() * -100\n",
    "            if CUDA:\n",
    "                mlm_labels = mlm_labels.cuda()\n",
    "            \n",
    "            # Set target labels for masked positions and apply masks to input\n",
    "            mlm_labels.scatter_(1, mask, target_words)\n",
    "            W_ids.scatter_(1, mask, MASK_IDX)\n",
    "        \n",
    "        # Get text representations from Longformer\n",
    "        # WARNING: Using self.distilbert instead of inherited model - this ignores pretrained weights!\n",
    "        outputs = self.encoder(W_ids, attention_mask=W_mask)\n",
    "        seq_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        pooled_output = outputs.pooler_output    # [batch_size, hidden_size]\n",
    "        \n",
    "        # Masked Language Modeling loss calculation\n",
    "        if use_mlm:\n",
    "            prediction_logits = self.vocab_transform(seq_output)\n",
    "            prediction_logits = gelu(prediction_logits)\n",
    "            prediction_logits = self.vocab_layer_norm(prediction_logits)\n",
    "            prediction_logits = self.vocab_projector(prediction_logits)\n",
    "            \n",
    "            mlm_loss = CrossEntropyLoss()(\n",
    "                prediction_logits.view(-1, self.vocab_size), \n",
    "                mlm_labels.view(-1)\n",
    "            )\n",
    "        else:\n",
    "            mlm_loss = 0.0\n",
    "        \n",
    "        # CONFOUNDER INTEGRATION: Key innovation of this version\n",
    "        # Ensure confounders have compatible data type with text representations\n",
    "        C = C.to(pooled_output.dtype)\n",
    "        \n",
    "        # Concatenate text representations with confounder variables\n",
    "        # This creates a joint representation: [text_features, confounder1, confounder2]\n",
    "        inputs = torch.cat((pooled_output, C), 1)  # [batch_size, hidden_size + 2]\n",
    "        \n",
    "        # Propensity score prediction using combined text+confounder features\n",
    "        g = self.g_cls(inputs)\n",
    "        if Y is not None:  \n",
    "            # Binary cross-entropy loss for propensity score\n",
    "            g_loss = BCELoss()(g.flatten(), T.flatten().float())\n",
    "        else:\n",
    "            g_loss = 0.0\n",
    "\n",
    "        # Q-learning: Predict potential outcomes using text+confounder features\n",
    "        Q_logits_T0 = self.Q_cls['0'](inputs)  # Outcome if treated with T=0\n",
    "        Q_logits_T1 = self.Q_cls['1'](inputs)  # Outcome if treated with T=1\n",
    "\n",
    "        # Q-learning loss: Only compute loss for observed treatment-outcome pairs\n",
    "        # This implements the fundamental Q-learning principle: learn from observed data\n",
    "        if Y is not None:\n",
    "            Y = Y.float()\n",
    "            T0_indices = (T == 0).nonzero().squeeze()  # Samples that received treatment 0\n",
    "            T1_indices = (T == 1).nonzero().squeeze()  # Samples that received treatment 1\n",
    "            \n",
    "            # Compute outcome prediction loss only for corresponding treatment groups\n",
    "            # This prevents the model from learning incorrect treatment-outcome associations\n",
    "            if T1_indices.nelement() != 0:\n",
    "                Q_loss_T1 = MSELoss()(\n",
    "                    Q_logits_T1.flatten()[T1_indices], \n",
    "                    Y.float()[T1_indices]\n",
    "                )\n",
    "            else:\n",
    "                Q_loss_T1 = 0.0\n",
    "                \n",
    "            if T0_indices.nelement() != 0:\n",
    "                Q_loss_T0 = MSELoss()(\n",
    "                    Q_logits_T0.flatten()[T0_indices], \n",
    "                    Y.float()[T0_indices]\n",
    "                )\n",
    "            else:\n",
    "                Q_loss_T0 = 0.0\n",
    "\n",
    "            Q_loss = Q_loss_T0 + Q_loss_T1\n",
    "        else:\n",
    "            Q_loss = 0.0\n",
    "\n",
    "        Q0 = Q_logits_T0\n",
    "        Q1 = Q_logits_T1\n",
    "        \n",
    "        return g, Q0, Q1, g_loss, Q_loss, mlm_loss\n",
    "\n",
    "\n",
    "class CausalBertWrapper:\n",
    "    \"\"\"\n",
    "    Enhanced wrapper class for training and inference with CausalBert v2.\n",
    "    Handles confounder preprocessing and integration with text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g_weight=1.0, Q_weight=1.0, mlm_weight=1.0, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize CausalBert wrapper with multi-task loss weighting.\n",
    "        \n",
    "        Args:\n",
    "            g_weight: Weight for propensity score loss (higher = more focus on treatment prediction)\n",
    "            Q_weight: Weight for Q-learning loss (higher = more focus on outcome prediction)\n",
    "            mlm_weight: Weight for masked language modeling loss (higher = more language understanding)\n",
    "            batch_size: Batch size for training and inference\n",
    "        \"\"\"\n",
    "        # Load pretrained Longformer and extend with causal inference capabilities\n",
    "        self.model = CausalBert.from_pretrained(\n",
    "            \"allenai/longformer-base-4096\",\n",
    "            num_labels=2,  # Binary treatment\n",
    "            output_attentions=False,  # Save memory\n",
    "            output_hidden_states=False  # Save memory\n",
    "        )\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        if CUDA:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        # Multi-task learning loss weights\n",
    "        self.loss_weights = {\n",
    "            'g': g_weight,      # Propensity score learning\n",
    "            'Q': Q_weight,      # Outcome prediction learning\n",
    "            'mlm': mlm_weight   # Language understanding preservation\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, texts, confounds, treatments, outcomes,\n",
    "              learning_rate=2e-5, epochs=3, filename=None):\n",
    "        \"\"\"\n",
    "        Train the CausalBert model with integrated confounder variables.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text documents (e.g., medical records, reviews, patents)\n",
    "            confounds: List/array of confounder variables [n_samples, 2] (e.g., [age, income])\n",
    "            treatments: List of treatment indicators (0 or 1)\n",
    "            outcomes: List of outcome values (continuous or binary)\n",
    "            learning_rate: Learning rate for AdamW optimizer\n",
    "            epochs: Number of training epochs\n",
    "            filename: Optional CSV filename to save training loss progression\n",
    "        \n",
    "        Returns:\n",
    "            Trained model\n",
    "        \"\"\"\n",
    "        dataloader = self.build_dataloader(texts, confounds, treatments, outcomes)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "        # Setup AdamW optimizer with learning rate scheduling\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "        total_steps = len(dataloader) * epochs\n",
    "        warmup_steps = total_steps * 0.1  # 10% warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps, \n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        avg_loss = []\n",
    "        \n",
    "        # Training loop with multi-task loss optimization\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            self.model.train()\n",
    "            \n",
    "            for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                # Move batch to GPU if available\n",
    "                if CUDA: \n",
    "                    batch = tuple(x.cuda() for x in batch)\n",
    "                \n",
    "                W_ids, W_len, W_mask, C, T, Y = batch\n",
    "                \n",
    "                # Forward pass with multi-task learning\n",
    "                self.model.zero_grad()\n",
    "                g, Q0, Q1, g_loss, Q_loss, mlm_loss = self.model(\n",
    "                    W_ids, W_len, W_mask, C, T, Y\n",
    "                )\n",
    "                \n",
    "                # Weighted combination of all losses\n",
    "                total_loss = (self.loss_weights['g'] * g_loss + \n",
    "                             self.loss_weights['Q'] * Q_loss + \n",
    "                             self.loss_weights['mlm'] * mlm_loss)\n",
    " \n",
    "                # Backward pass and optimization step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Track Q-learning loss specifically (main task)\n",
    "                losses.append(Q_loss.detach().cpu().item())\n",
    "                \n",
    "            avg_loss.append(np.mean(losses))\n",
    "        \n",
    "        # Save training loss progression if filename provided\n",
    "        if filename:\n",
    "            loss_df = pd.DataFrame(avg_loss, columns=[\"loss\"])\n",
    "            loss_df.to_csv(filename, index=False)\n",
    "            \n",
    "        return self.model\n",
    "\n",
    "    def inference(self, texts, confounds, treatment=None, outcome=None):\n",
    "        \"\"\"\n",
    "        Perform inference with trained model to get predictions.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text documents\n",
    "            confounds: List/array of confounder variables\n",
    "            treatment: Optional treatment indicators (for evaluation)\n",
    "            outcome: Optional outcome values (for evaluation)\n",
    "        \n",
    "        Returns:\n",
    "            Q0s: Predicted outcomes under treatment 0\n",
    "            Q1s: Predicted outcomes under treatment 1\n",
    "            Gs: Predicted propensity scores\n",
    "            Ts: Treatment indicators\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dataloader = self.build_dataloader(\n",
    "            texts, confounds, \n",
    "            treatments=treatment, outcomes=outcome,\n",
    "            sampler='sequential'  # Deterministic order for inference\n",
    "        )\n",
    "        \n",
    "        # Collect all predictions\n",
    "        Q0s, Q1s, Ys, Gs, Ts = [], [], [], [], []\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                if CUDA: \n",
    "                    batch = tuple(x.cuda() for x in batch)\n",
    "                \n",
    "                W_ids, W_len, W_mask, C, T, Y = batch\n",
    "                \n",
    "                # Forward pass without MLM (faster for inference)\n",
    "                g, Q0, Q1, _, _, _ = self.model(\n",
    "                    W_ids, W_len, W_mask, C, T, use_mlm=False\n",
    "                )\n",
    "                \n",
    "                # Convert predictions to CPU and collect\n",
    "                Q0s += Q0.flatten().detach().cpu().numpy().tolist()\n",
    "                Q1s += Q1.flatten().detach().cpu().numpy().tolist()\n",
    "                Ys += Y.flatten().detach().cpu().numpy().tolist()\n",
    "                Gs += g.flatten().detach().cpu().numpy().tolist()\n",
    "                Ts += T.flatten().detach().cpu().numpy().tolist()\n",
    "                \n",
    "        return Q0s, Q1s, Gs, Ts\n",
    "\n",
    "    def ATE(self, C, W, df, filename, Y=None, T=None, names=None, dates=None, platt_scaling=False):\n",
    "        \"\"\"\n",
    "        Compute Average Treatment Effect (ATE) using the trained model. Note that we no longer claim causality in the newest version of the manual script.\n",
    "\n",
    "        This function also returns the estimated citations and  gender (group indicator) propensity. The ATE number should only be used in cases with randomized gender (group) assignment.\n",
    "        \n",
    "        ATE represents the average difference in outcomes if everyone received treatment\n",
    "        vs. if everyone received control, accounting for confounders and selection bias.\n",
    "        \n",
    "        Args:\n",
    "            C: Confounder variables\n",
    "            W: Text data\n",
    "            df: DataFrame to store detailed results\n",
    "            filename: Output CSV filename for results\n",
    "            Y: True outcomes (optional)\n",
    "            T: True treatments (optional)\n",
    "            names: Optional sample names/IDs\n",
    "            dates: Optional dates\n",
    "            platt_scaling: Whether to apply Platt scaling to propensity scores\n",
    "        \n",
    "        Returns:\n",
    "            Estimated Average Treatment Effect among the treated (ATT)\n",
    "        \"\"\"\n",
    "        Q0s, Q1s, Gs, Ts = self.inference(W, C, treatment=T, outcome=Y)\n",
    "        \n",
    "        # Store comprehensive results in dataframe\n",
    "        df[\"Q0\"] = Q0s           # Predicted outcome under control (T=0)\n",
    "        df[\"Q1\"] = Q1s           # Predicted outcome under treatment (T=1)\n",
    "        df[\"Gs\"] = Gs            # Propensity scores P(T=1|X,C)\n",
    "        df[\"true_out\"] = Y       # True observed outcomes\n",
    "        df[\"T\"] = T              # True treatment assignments\n",
    "        \n",
    "        # Save results for further analysis\n",
    "        df[[\"patent_id\", \"Q0\", \"Q1\", \"Gs\", \"true_out\", \"T\"]].to_csv(filename, index=False)\n",
    "        \n",
    "        # Compute Average Treatment Effect on the Treated (ATT)\n",
    "        # ATT = E[Q1 - Q0 | T=1] - effect among those who actually received treatment\n",
    "        treatment_effect = np.array(Q1s) - np.array(Q0s)  # Individual treatment effects\n",
    "        treated_mask = np.array(Ts)  # Binary mask for treated individuals\n",
    "        \n",
    "        # Weighted average of treatment effects among treated individuals\n",
    "        att = np.mean(np.multiply(treatment_effect, treated_mask)) / np.sum(treated_mask)\n",
    "        \n",
    "        return att\n",
    "    \n",
    "    def build_dataloader(self, texts, confounds, treatments=None, outcomes=None,\n",
    "                        tokenizer=None, sampler='random'):\n",
    "        \"\"\"\n",
    "        Build PyTorch DataLoader with proper confounder preprocessing.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text documents\n",
    "            confounds: Confounder variables (will be converted to numpy array)\n",
    "            treatments: Treatment indicators (optional, filled with -1 if None)\n",
    "            outcomes: Outcome values (optional, filled with -1 if None)\n",
    "            tokenizer: Pretrained tokenizer (optional)\n",
    "            sampler: 'random' or 'sequential' data sampling\n",
    "        \n",
    "        Returns:\n",
    "            PyTorch DataLoader with batched, tokenized data\n",
    "        \"\"\"\n",
    "        # Fill missing values with dummy data for inference\n",
    "        if treatments is None:\n",
    "            treatments = [-1 for _ in range(len(confounds))]\n",
    "        if outcomes is None:\n",
    "            outcomes = [-1 for _ in range(len(treatments))]\n",
    "\n",
    "        # Load Longformer tokenizer if not provided\n",
    "        if tokenizer is None:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                'allenai/longformer-base-4096', do_lower_case=True\n",
    "            )\n",
    "\n",
    "        # Ensure confounders are numpy array for consistent processing\n",
    "        confounds = np.asarray(confounds)\n",
    "        \n",
    "        # Process each sample: tokenize text and collect confounders\n",
    "        out = defaultdict(list)\n",
    "        for i, (W, C, T, Y) in enumerate(zip(texts, confounds, treatments, outcomes)):\n",
    "            # Tokenize text with proper truncation and padding\n",
    "            encoded_sent = tokenizer.encode_plus(\n",
    "                W, \n",
    "                add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "                max_length=512,           # Longformer context limit\n",
    "                truncation=True,          # Truncate if longer\n",
    "                pad_to_max_length=True    # Pad if shorter\n",
    "            )\n",
    "\n",
    "            # Store tokenized text data\n",
    "            out['W_ids'].append(encoded_sent['input_ids'])\n",
    "            out['W_mask'].append(encoded_sent['attention_mask'])\n",
    "            out['W_len'].append(sum(encoded_sent['attention_mask']))  # Actual sequence length\n",
    "            \n",
    "            # Store targets and confounders\n",
    "            out['Y'].append(Y)  # Outcomes\n",
    "            out['T'].append(T)  # Treatments\n",
    "            out['C'].append(C)  # Confounder variables (should be 2-dimensional)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        data = tuple(torch.tensor(out[x]) for x in ['W_ids', 'W_len', 'W_mask', 'C', 'T', 'Y'])\n",
    "        dataset = TensorDataset(*data)\n",
    "        \n",
    "        # Choose appropriate sampler\n",
    "        sampler = RandomSampler(dataset) if sampler == 'random' else SequentialSampler(dataset)\n",
    "        dataloader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n",
    "\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9985e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set max GPU usage to avoid system crashing\n",
    "torch.cuda.set_per_process_memory_fraction(1.0, 0)\n",
    "#clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fadd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "Total:    79.1 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "print('Total:   ', round(total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac2df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7781c040a04e44c3b0274e7867847922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec1b77f5c08417db9bfd56c7f78c107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of CausalBert were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.Q_cls.0.0.bias', 'longformer.Q_cls.0.0.weight', 'longformer.Q_cls.0.2.bias', 'longformer.Q_cls.0.2.weight', 'longformer.Q_cls.1.0.bias', 'longformer.Q_cls.1.0.weight', 'longformer.Q_cls.1.2.bias', 'longformer.Q_cls.1.2.weight', 'longformer.distilbert.embeddings.LayerNorm.bias', 'longformer.distilbert.embeddings.LayerNorm.weight', 'longformer.distilbert.embeddings.position_embeddings.weight', 'longformer.distilbert.embeddings.token_type_embeddings.weight', 'longformer.distilbert.embeddings.word_embeddings.weight', 'longformer.distilbert.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.0.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.0.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.0.attention.self.key.bias', 'longformer.distilbert.encoder.layer.0.attention.self.key.weight', 'longformer.distilbert.encoder.layer.0.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.0.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.0.attention.self.query.bias', 'longformer.distilbert.encoder.layer.0.attention.self.query.weight', 'longformer.distilbert.encoder.layer.0.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.0.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.0.attention.self.value.bias', 'longformer.distilbert.encoder.layer.0.attention.self.value.weight', 'longformer.distilbert.encoder.layer.0.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.0.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.0.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.0.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.0.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.0.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.0.output.dense.bias', 'longformer.distilbert.encoder.layer.0.output.dense.weight', 'longformer.distilbert.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.1.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.1.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.1.attention.self.key.bias', 'longformer.distilbert.encoder.layer.1.attention.self.key.weight', 'longformer.distilbert.encoder.layer.1.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.1.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.1.attention.self.query.bias', 'longformer.distilbert.encoder.layer.1.attention.self.query.weight', 'longformer.distilbert.encoder.layer.1.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.1.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.1.attention.self.value.bias', 'longformer.distilbert.encoder.layer.1.attention.self.value.weight', 'longformer.distilbert.encoder.layer.1.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.1.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.1.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.1.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.1.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.1.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.1.output.dense.bias', 'longformer.distilbert.encoder.layer.1.output.dense.weight', 'longformer.distilbert.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.10.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.10.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.10.attention.self.key.bias', 'longformer.distilbert.encoder.layer.10.attention.self.key.weight', 'longformer.distilbert.encoder.layer.10.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.10.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.10.attention.self.query.bias', 'longformer.distilbert.encoder.layer.10.attention.self.query.weight', 'longformer.distilbert.encoder.layer.10.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.10.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.10.attention.self.value.bias', 'longformer.distilbert.encoder.layer.10.attention.self.value.weight', 'longformer.distilbert.encoder.layer.10.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.10.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.10.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.10.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.10.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.10.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.10.output.dense.bias', 'longformer.distilbert.encoder.layer.10.output.dense.weight', 'longformer.distilbert.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.11.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.11.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.11.attention.self.key.bias', 'longformer.distilbert.encoder.layer.11.attention.self.key.weight', 'longformer.distilbert.encoder.layer.11.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.11.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.11.attention.self.query.bias', 'longformer.distilbert.encoder.layer.11.attention.self.query.weight', 'longformer.distilbert.encoder.layer.11.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.11.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.11.attention.self.value.bias', 'longformer.distilbert.encoder.layer.11.attention.self.value.weight', 'longformer.distilbert.encoder.layer.11.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.11.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.11.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.11.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.11.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.11.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.11.output.dense.bias', 'longformer.distilbert.encoder.layer.11.output.dense.weight', 'longformer.distilbert.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.2.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.2.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.2.attention.self.key.bias', 'longformer.distilbert.encoder.layer.2.attention.self.key.weight', 'longformer.distilbert.encoder.layer.2.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.2.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.2.attention.self.query.bias', 'longformer.distilbert.encoder.layer.2.attention.self.query.weight', 'longformer.distilbert.encoder.layer.2.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.2.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.2.attention.self.value.bias', 'longformer.distilbert.encoder.layer.2.attention.self.value.weight', 'longformer.distilbert.encoder.layer.2.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.2.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.2.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.2.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.2.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.2.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.2.output.dense.bias', 'longformer.distilbert.encoder.layer.2.output.dense.weight', 'longformer.distilbert.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.3.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.3.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.3.attention.self.key.bias', 'longformer.distilbert.encoder.layer.3.attention.self.key.weight', 'longformer.distilbert.encoder.layer.3.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.3.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.3.attention.self.query.bias', 'longformer.distilbert.encoder.layer.3.attention.self.query.weight', 'longformer.distilbert.encoder.layer.3.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.3.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.3.attention.self.value.bias', 'longformer.distilbert.encoder.layer.3.attention.self.value.weight', 'longformer.distilbert.encoder.layer.3.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.3.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.3.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.3.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.3.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.3.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.3.output.dense.bias', 'longformer.distilbert.encoder.layer.3.output.dense.weight', 'longformer.distilbert.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.4.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.4.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.4.attention.self.key.bias', 'longformer.distilbert.encoder.layer.4.attention.self.key.weight', 'longformer.distilbert.encoder.layer.4.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.4.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.4.attention.self.query.bias', 'longformer.distilbert.encoder.layer.4.attention.self.query.weight', 'longformer.distilbert.encoder.layer.4.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.4.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.4.attention.self.value.bias', 'longformer.distilbert.encoder.layer.4.attention.self.value.weight', 'longformer.distilbert.encoder.layer.4.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.4.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.4.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.4.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.4.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.4.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.4.output.dense.bias', 'longformer.distilbert.encoder.layer.4.output.dense.weight', 'longformer.distilbert.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.5.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.5.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.5.attention.self.key.bias', 'longformer.distilbert.encoder.layer.5.attention.self.key.weight', 'longformer.distilbert.encoder.layer.5.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.5.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.5.attention.self.query.bias', 'longformer.distilbert.encoder.layer.5.attention.self.query.weight', 'longformer.distilbert.encoder.layer.5.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.5.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.5.attention.self.value.bias', 'longformer.distilbert.encoder.layer.5.attention.self.value.weight', 'longformer.distilbert.encoder.layer.5.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.5.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.5.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.5.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.5.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.5.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.5.output.dense.bias', 'longformer.distilbert.encoder.layer.5.output.dense.weight', 'longformer.distilbert.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.6.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.6.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.6.attention.self.key.bias', 'longformer.distilbert.encoder.layer.6.attention.self.key.weight', 'longformer.distilbert.encoder.layer.6.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.6.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.6.attention.self.query.bias', 'longformer.distilbert.encoder.layer.6.attention.self.query.weight', 'longformer.distilbert.encoder.layer.6.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.6.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.6.attention.self.value.bias', 'longformer.distilbert.encoder.layer.6.attention.self.value.weight', 'longformer.distilbert.encoder.layer.6.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.6.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.6.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.6.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.6.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.6.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.6.output.dense.bias', 'longformer.distilbert.encoder.layer.6.output.dense.weight', 'longformer.distilbert.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.7.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.7.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.7.attention.self.key.bias', 'longformer.distilbert.encoder.layer.7.attention.self.key.weight', 'longformer.distilbert.encoder.layer.7.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.7.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.7.attention.self.query.bias', 'longformer.distilbert.encoder.layer.7.attention.self.query.weight', 'longformer.distilbert.encoder.layer.7.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.7.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.7.attention.self.value.bias', 'longformer.distilbert.encoder.layer.7.attention.self.value.weight', 'longformer.distilbert.encoder.layer.7.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.7.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.7.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.7.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.7.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.7.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.7.output.dense.bias', 'longformer.distilbert.encoder.layer.7.output.dense.weight', 'longformer.distilbert.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.8.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.8.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.8.attention.self.key.bias', 'longformer.distilbert.encoder.layer.8.attention.self.key.weight', 'longformer.distilbert.encoder.layer.8.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.8.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.8.attention.self.query.bias', 'longformer.distilbert.encoder.layer.8.attention.self.query.weight', 'longformer.distilbert.encoder.layer.8.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.8.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.8.attention.self.value.bias', 'longformer.distilbert.encoder.layer.8.attention.self.value.weight', 'longformer.distilbert.encoder.layer.8.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.8.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.8.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.8.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.8.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.8.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.8.output.dense.bias', 'longformer.distilbert.encoder.layer.8.output.dense.weight', 'longformer.distilbert.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.9.attention.output.dense.bias', 'longformer.distilbert.encoder.layer.9.attention.output.dense.weight', 'longformer.distilbert.encoder.layer.9.attention.self.key.bias', 'longformer.distilbert.encoder.layer.9.attention.self.key.weight', 'longformer.distilbert.encoder.layer.9.attention.self.key_global.bias', 'longformer.distilbert.encoder.layer.9.attention.self.key_global.weight', 'longformer.distilbert.encoder.layer.9.attention.self.query.bias', 'longformer.distilbert.encoder.layer.9.attention.self.query.weight', 'longformer.distilbert.encoder.layer.9.attention.self.query_global.bias', 'longformer.distilbert.encoder.layer.9.attention.self.query_global.weight', 'longformer.distilbert.encoder.layer.9.attention.self.value.bias', 'longformer.distilbert.encoder.layer.9.attention.self.value.weight', 'longformer.distilbert.encoder.layer.9.attention.self.value_global.bias', 'longformer.distilbert.encoder.layer.9.attention.self.value_global.weight', 'longformer.distilbert.encoder.layer.9.intermediate.dense.bias', 'longformer.distilbert.encoder.layer.9.intermediate.dense.weight', 'longformer.distilbert.encoder.layer.9.output.LayerNorm.bias', 'longformer.distilbert.encoder.layer.9.output.LayerNorm.weight', 'longformer.distilbert.encoder.layer.9.output.dense.bias', 'longformer.distilbert.encoder.layer.9.output.dense.weight', 'longformer.distilbert.pooler.dense.bias', 'longformer.distilbert.pooler.dense.weight', 'longformer.g_cls.0.bias', 'longformer.g_cls.0.weight', 'longformer.vocab_layer_norm.bias', 'longformer.vocab_layer_norm.weight', 'longformer.vocab_projector.bias', 'longformer.vocab_projector.weight', 'longformer.vocab_transform.bias', 'longformer.vocab_transform.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc12ac9f0e44e139e49baa697cb9f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b645ed84b04ed2849f910da04430a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b328252d99e4a10a1a627aaa29280df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_165/3583234254.py:295: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  data = (torch.tensor(out[x]) for x in ['W_ids', 'W_len', 'W_mask', 'C', 'T', 'Y'])\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/28536 [00:00<?, ?it/s]/tmp/ipykernel_165/3583234254.py:74: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  mask = (mask_class(W_len.shape).uniform_() * W_len.float()).long() + 1 # + 1 to avoid CLS\n",
      "100%|██████████| 28536/28536 [4:11:55<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:11:59<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:12:01<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:12:03<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:12:04<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:11:58<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:11:57<00:00,  1.89it/s]  \n",
      "100%|██████████| 28536/28536 [4:11:59<00:00,  1.89it/s]  \n",
      " 42%|████▏     | 11866/28536 [1:44:47<2:27:04,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "#run the model fitting code to get the text quality, group propensity, and model. the df should include text, group indicator (gender), and target (citation counts)\n",
    "    cb = CausalBertWrapper(batch_size=16,  \n",
    "        g_weight=1, Q_weight=1, mlm_weight=1)\n",
    "#fit the model\n",
    "    cb.train(df['text'], confounders, df['group'], df['target'], epochs=20,filename=\"loss file path\")\n",
    "#get the model outputs (only use ATE numbers when group indicators are randomly assigned)\n",
    "    cb.ATE(confounder, df['text'],df,\"numerical output path\",Y=df['target'],T=df['group'] ,platt_scaling=True)\n",
    "#save model\n",
    "    file_name = 'model path'\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(cb.model, file)\n",
    "        print(f'Object successfully saved to \"{file_name}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
